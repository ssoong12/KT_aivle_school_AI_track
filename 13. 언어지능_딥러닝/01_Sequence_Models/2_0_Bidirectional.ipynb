{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"peI31xdG8j0C"},"source":["# Sequence Models & How to use them\n","\n","**학습목표**\n","1. Bidirectional layer를 이해한다.\n","2. Bidirectional layer를 사용할 줄 안다.\n","\n","-----------------\n","* Thanks to : 한기영 대표님 @ Data Insight"]},{"cell_type":"markdown","metadata":{"id":"jnCv9u_Wy9e1"},"source":["## 1.환경 및 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"O1uL0UIEzBPw"},"source":["### Import Packages"]},{"cell_type":"code","metadata":{"id":"GVvCef6A8uv6","executionInfo":{"status":"ok","timestamp":1680076697922,"user_tz":-540,"elapsed":6217,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}}},"source":["#라이브러리들을 불러오자.\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.utils import plot_model\n","from sklearn.metrics import mean_squared_error"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"kcjRzAOuGKyC"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHy5s1OOzItm"},"source":["### Data Loading"]},{"cell_type":"code","metadata":{"id":"3GgFpOB4zQ7h","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"ok","timestamp":1680076719486,"user_tz":-540,"elapsed":26,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"1d78745b-30dd-4d05-a132-fca37edd6bd7"},"source":["url = 'https://raw.githubusercontent.com/RayleighKim/Example_datasets/master/Stock_Edwards_Lifesciences_corporation.csv'\n","\n","# 판다스로 데이터를 불러오시오.\n","data = pd.read_csv(url)\n","data.tail(10)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           Date        Open        High         Low       Close   Adj_Close  \\\n","4382  2017/8/25  112.949997  113.400002  111.720001  111.760002  111.760002   \n","4383  2017/8/28  112.559998  113.449997  112.470001  113.110001  113.110001   \n","4384  2017/8/29  112.320000  113.250000  111.739998  111.830002  111.830002   \n","4385  2017/8/30  112.019997  112.989998  111.529999  112.690002  112.690002   \n","4386  2017/8/31  112.820000  113.790001  112.440002  113.660004  113.660004   \n","4387   2017/9/1  113.790001  114.099998  112.790001  113.309998  113.309998   \n","4388   2017/9/5  112.519997  113.529999  111.160004  111.870003  111.870003   \n","4389   2017/9/6  112.029999  112.489998  110.250000  112.230003  112.230003   \n","4390   2017/9/7  112.459999  112.900002  112.000000  112.339996  112.339996   \n","4391   2017/9/8  112.300003  114.790001  112.010002  113.190002  113.190002   \n","\n","       Volume  \n","4382  1206400  \n","4383  1050200  \n","4384  1366000  \n","4385   929200  \n","4386  1091400  \n","4387   950000  \n","4388  1805200  \n","4389  2136700  \n","4390  1251600  \n","4391  1611700  "],"text/html":["\n","  <div id=\"df-feccdabe-a095-4012-bff4-b82bb71bb403\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Adj_Close</th>\n","      <th>Volume</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4382</th>\n","      <td>2017/8/25</td>\n","      <td>112.949997</td>\n","      <td>113.400002</td>\n","      <td>111.720001</td>\n","      <td>111.760002</td>\n","      <td>111.760002</td>\n","      <td>1206400</td>\n","    </tr>\n","    <tr>\n","      <th>4383</th>\n","      <td>2017/8/28</td>\n","      <td>112.559998</td>\n","      <td>113.449997</td>\n","      <td>112.470001</td>\n","      <td>113.110001</td>\n","      <td>113.110001</td>\n","      <td>1050200</td>\n","    </tr>\n","    <tr>\n","      <th>4384</th>\n","      <td>2017/8/29</td>\n","      <td>112.320000</td>\n","      <td>113.250000</td>\n","      <td>111.739998</td>\n","      <td>111.830002</td>\n","      <td>111.830002</td>\n","      <td>1366000</td>\n","    </tr>\n","    <tr>\n","      <th>4385</th>\n","      <td>2017/8/30</td>\n","      <td>112.019997</td>\n","      <td>112.989998</td>\n","      <td>111.529999</td>\n","      <td>112.690002</td>\n","      <td>112.690002</td>\n","      <td>929200</td>\n","    </tr>\n","    <tr>\n","      <th>4386</th>\n","      <td>2017/8/31</td>\n","      <td>112.820000</td>\n","      <td>113.790001</td>\n","      <td>112.440002</td>\n","      <td>113.660004</td>\n","      <td>113.660004</td>\n","      <td>1091400</td>\n","    </tr>\n","    <tr>\n","      <th>4387</th>\n","      <td>2017/9/1</td>\n","      <td>113.790001</td>\n","      <td>114.099998</td>\n","      <td>112.790001</td>\n","      <td>113.309998</td>\n","      <td>113.309998</td>\n","      <td>950000</td>\n","    </tr>\n","    <tr>\n","      <th>4388</th>\n","      <td>2017/9/5</td>\n","      <td>112.519997</td>\n","      <td>113.529999</td>\n","      <td>111.160004</td>\n","      <td>111.870003</td>\n","      <td>111.870003</td>\n","      <td>1805200</td>\n","    </tr>\n","    <tr>\n","      <th>4389</th>\n","      <td>2017/9/6</td>\n","      <td>112.029999</td>\n","      <td>112.489998</td>\n","      <td>110.250000</td>\n","      <td>112.230003</td>\n","      <td>112.230003</td>\n","      <td>2136700</td>\n","    </tr>\n","    <tr>\n","      <th>4390</th>\n","      <td>2017/9/7</td>\n","      <td>112.459999</td>\n","      <td>112.900002</td>\n","      <td>112.000000</td>\n","      <td>112.339996</td>\n","      <td>112.339996</td>\n","      <td>1251600</td>\n","    </tr>\n","    <tr>\n","      <th>4391</th>\n","      <td>2017/9/8</td>\n","      <td>112.300003</td>\n","      <td>114.790001</td>\n","      <td>112.010002</td>\n","      <td>113.190002</td>\n","      <td>113.190002</td>\n","      <td>1611700</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-feccdabe-a095-4012-bff4-b82bb71bb403')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-feccdabe-a095-4012-bff4-b82bb71bb403 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-feccdabe-a095-4012-bff4-b82bb71bb403');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"wW0d0eGmCYJM"},"source":["##3.데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"pUTxBL4EbG3t"},"source":["### Date 컬럼을 제거하시오."]},{"cell_type":"code","metadata":{"id":"Jdpd5KXobKi8","executionInfo":{"status":"ok","timestamp":1680076735813,"user_tz":-540,"elapsed":4,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}}},"source":["data.drop('Date', axis = 1, inplace = True)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdjDoyE-t0Ms"},"source":["## 4.Sequence 데이터 구조로 만들기"]},{"cell_type":"markdown","metadata":{"id":"zZf0MntMBpGf"},"source":["**조건**\n","* 내일의 Close를 예측할 것이다. 이를 Y로 둘 것.\n","* timestep은 10주를 본다. (주식은 5일이 1주일)\n","* 맞추어 전처리\n","\n","**데이터 분할 규칙**\n","* 가장 최근 1주일을 테스트 데이터로 둔다.\n","* 테스트 데이터를 제외하고, 가장 최근 2주를 벨리데이션 데이터로 둔다."]},{"cell_type":"code","metadata":{"id":"PIwBTERUZ69X","executionInfo":{"status":"ok","timestamp":1680076827783,"user_tz":-540,"elapsed":264,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}}},"source":["# 판다스 데이터 프레임을 넘파이로 옮김.\n","data = data.values"],"execution_count":5,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"spR-gE4xYfHu","executionInfo":{"status":"ok","timestamp":1680076832302,"user_tz":-540,"elapsed":4,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"351c8a40-342e-45fb-9939-da0ddc120320"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[3.81250000e+00, 4.15625000e+00, 3.81250000e+00, 4.12500000e+00,\n","        4.12500000e+00, 3.67560000e+06],\n","       [4.12500000e+00, 4.12500000e+00, 4.00000000e+00, 4.01562500e+00,\n","        4.01562500e+00, 1.07760000e+06],\n","       [4.00000000e+00, 4.03125000e+00, 3.95312500e+00, 4.00000000e+00,\n","        4.00000000e+00, 4.37200000e+05],\n","       ...,\n","       [1.12029999e+02, 1.12489998e+02, 1.10250000e+02, 1.12230003e+02,\n","        1.12230003e+02, 2.13670000e+06],\n","       [1.12459999e+02, 1.12900002e+02, 1.12000000e+02, 1.12339996e+02,\n","        1.12339996e+02, 1.25160000e+06],\n","       [1.12300003e+02, 1.14790001e+02, 1.12010002e+02, 1.13190002e+02,\n","        1.13190002e+02, 1.61170000e+06]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pD95ez9tYgt3","executionInfo":{"status":"ok","timestamp":1680076840630,"user_tz":-540,"elapsed":345,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"dd0f785f-0c55-4910-987e-06d527f9f229"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4392, 6)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"6ptn-ht0LNj-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680077088597,"user_tz":-540,"elapsed":283,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"47450770-cfbe-4101-a708-80c2c2172f7a"},"source":["# 여기에 의미있는 기간(timestep을 지정해 봅시다.)\n","timestep = 50\n","\n","# x의 데이터 구조를 3차원으로 만들기\n","x = np.array([data[i : i + timestep] for i in range( len(data) - timestep) ])\n","y = np.array([data[i + timestep, -1] for i in range( len(data) - timestep) ])\n","\n","# train, val, test set 분리\n","x_train, x_val, x_test = x[:-15], x[-15:-5], x[-5:]\n","y_train, y_val, y_test = y[:-15], y[-15:-5], y[-5:]\n","\n","## 모양 확인 필수\n","print(x.shape, y.shape)\n","print('-------------------------------')\n","print(x_train.shape, y_train.shape)\n","print(x_val.shape, y_val.shape)\n","print(x_test.shape, y_test.shape)\n","print('-------------------------------')\n","print('[ # , timestep, feature수 ] <-- 데이터의 구조 : ')\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(4342, 50, 6) (4342,)\n","-------------------------------\n","(4327, 50, 6) (4327,)\n","(10, 50, 6) (10,)\n","(5, 50, 6) (5,)\n","-------------------------------\n","[ # , timestep, feature수 ] <-- 데이터의 구조 : \n"]}]},{"cell_type":"markdown","metadata":{"id":"Hp8vF8ugCYDI"},"source":["데이터의 구조 : [n, timestep, feature수]"]},{"cell_type":"markdown","metadata":{"id":"Xw1rhqx0LME4"},"source":["# 같이 해보기\n","**[참고링크](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)**\n","1. 적절한 인풋 레이어를 구성할 것\n","2. Conv1D\n","    * 16개 필터 사용할 것\n","    * window_size(filter_size)는 5일\n","    * activation은 swish\n","3. MaxPool1D\n","    * 필터 사이즈(window size)는 2일\n","4. Bidirectional 레이어\n","    * LSTM, 히든스테이트 노드 32개\n","5. Bidirectional 레이어\n","    * LSTM, 히든스테이트 노드 32개\n","4. 플래튼\n","5. 회귀를 위한 적절한 아웃풋 레이어\n","\n"]},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense, Flatten, SimpleRNN\n","from tensorflow.keras.layers import Input, LSTM, GRU # 오전까지 배운 것\n","from tensorflow.keras.layers import Bidirectional, Conv1D, MaxPool1D # 오후에 배운 것\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.backend import clear_session"],"metadata":{"id":"jKcVyeypZtac","executionInfo":{"status":"ok","timestamp":1680077153204,"user_tz":-540,"elapsed":3,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_2-DfMPVbr-U","executionInfo":{"status":"ok","timestamp":1680077673845,"user_tz":-540,"elapsed":10,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"56ca666a-5cb2-4ce8-94c0-e1144daaa3ff"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4327, 50, 6)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"FA0U1fd3LL-K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680080327413,"user_tz":-540,"elapsed":1853,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"fd019db3-b70b-46aa-9b88-6e4b132af7a8"},"source":["####################\n","## Your Code Here ##\n","####################\n","# 1. 세션 클리어\n","clear_session()\n","\n","# 2. 모델 사슬처럼 엮기\n","# 인풋 레이어\n","il = Input(shape = (50, 6)) # 한 번에 50 시점, 시점당 6개 feature 존재\n","\n","# Conv1D\n","hl1 = Conv1D(filters = 16,  # 16 종류의 특징을 제작해 줘\n","            kernel_size = 5, # 한 번에 5시점씩 봐줘야 해. 일주일씩 보고 싶어서 커널사이즈 5\n","            activation = 'swish',\n","            padding='same')(il)\n","# MaxPool1D\n","hl2 = MaxPool1D(2)(hl1) # 필터 사이즈는 2일\n","\n","# Bidirectional 레이어\n","hl3 = Bidirectional(layer = LSTM(32, return_sequences=True))(hl2) # LSTM, 히든스테이트 노드 32개 앞에 거 다 참조하겠다.\n","hl4 = Bidirectional(layer = LSTM(32, return_sequences=True))(hl3) # LSTM, 히든스테이트 노드 32개 앞에 거 다 참조하겠다.\n","\n","# Flatten\n","hl5 = Flatten()(hl4)\n","\n","# 회귀를 위한 아웃풋 레이어\n","ol = Dense(1, activation = 'relu')(hl5)\n","\n","# 3. 시작, 끝 레이어 지정\n","model = Model(il, ol)\n","\n","# 4. 컴파일\n","model.compile(loss = 'mse', optimizer = 'adam')\n","\n","# 요약\n","model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 50, 6)]           0         \n","                                                                 \n"," conv1d (Conv1D)             (None, 50, 16)            496       \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, 25, 16)           0         \n"," )                                                               \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 25, 64)           12544     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 25, 64)           24832     \n"," nal)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 1600)              0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 1601      \n","                                                                 \n","=================================================================\n","Total params: 39,473\n","Trainable params: 39,473\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"_sWwR1CkLp3L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680081021186,"user_tz":-540,"elapsed":689732,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"f5da6534-ceff-4ac9-fb7e-1f2c0b216813"},"source":["## 학습도 시킬 것\n","hist = model.fit(x_train, y_train, epochs = 100, validation_data = (x_val, y_val), verbose = 1)\n"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","136/136 [==============================] - 20s 78ms/step - loss: 5992341307392.0000 - val_loss: 1691260026880.0000\n","Epoch 2/100\n","136/136 [==============================] - 8s 59ms/step - loss: 5991397588992.0000 - val_loss: 1690669154304.0000\n","Epoch 3/100\n","136/136 [==============================] - 7s 55ms/step - loss: 5990548766720.0000 - val_loss: 1690103316480.0000\n","Epoch 4/100\n","136/136 [==============================] - 5s 40ms/step - loss: 5989724061696.0000 - val_loss: 1689548226560.0000\n","Epoch 5/100\n","136/136 [==============================] - 8s 59ms/step - loss: 5988908793856.0000 - val_loss: 1688999821312.0000\n","Epoch 6/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5988099817472.0000 - val_loss: 1688451809280.0000\n","Epoch 7/100\n","136/136 [==============================] - 8s 61ms/step - loss: 5987298705408.0000 - val_loss: 1687907205120.0000\n","Epoch 8/100\n","136/136 [==============================] - 7s 49ms/step - loss: 5986496544768.0000 - val_loss: 1687364567040.0000\n","Epoch 9/100\n","136/136 [==============================] - 6s 45ms/step - loss: 5985695956992.0000 - val_loss: 1686823895040.0000\n","Epoch 10/100\n","136/136 [==============================] - 7s 49ms/step - loss: 5984899563520.0000 - val_loss: 1686280601600.0000\n","Epoch 11/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5984097402880.0000 - val_loss: 1685741371392.0000\n","Epoch 12/100\n","136/136 [==============================] - 6s 45ms/step - loss: 5983305203712.0000 - val_loss: 1685204369408.0000\n","Epoch 13/100\n","136/136 [==============================] - 8s 57ms/step - loss: 5982509858816.0000 - val_loss: 1684664090624.0000\n","Epoch 14/100\n","136/136 [==============================] - 7s 50ms/step - loss: 5981718183936.0000 - val_loss: 1684126564352.0000\n","Epoch 15/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5980923363328.0000 - val_loss: 1683589562368.0000\n","Epoch 16/100\n","136/136 [==============================] - 7s 53ms/step - loss: 5980129591296.0000 - val_loss: 1683052298240.0000\n","Epoch 17/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5979335819264.0000 - val_loss: 1682514247680.0000\n","Epoch 18/100\n","136/136 [==============================] - 7s 49ms/step - loss: 5978543620096.0000 - val_loss: 1681976983552.0000\n","Epoch 19/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5977750372352.0000 - val_loss: 1681440505856.0000\n","Epoch 20/100\n","136/136 [==============================] - 7s 49ms/step - loss: 5976958173184.0000 - val_loss: 1680903766016.0000\n","Epoch 21/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5976168071168.0000 - val_loss: 1680368467968.0000\n","Epoch 22/100\n","136/136 [==============================] - 6s 46ms/step - loss: 5975377444864.0000 - val_loss: 1679831597056.0000\n","Epoch 23/100\n","136/136 [==============================] - 7s 53ms/step - loss: 5974585769984.0000 - val_loss: 1679296954368.0000\n","Epoch 24/100\n","136/136 [==============================] - 8s 56ms/step - loss: 5973798813696.0000 - val_loss: 1678758772736.0000\n","Epoch 25/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5973005565952.0000 - val_loss: 1678226227200.0000\n","Epoch 26/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5972217036800.0000 - val_loss: 1677688832000.0000\n","Epoch 27/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5971423789056.0000 - val_loss: 1677154975744.0000\n","Epoch 28/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5970634211328.0000 - val_loss: 1676619153408.0000\n","Epoch 29/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5969842012160.0000 - val_loss: 1676083462144.0000\n","Epoch 30/100\n","136/136 [==============================] - 7s 50ms/step - loss: 5969054007296.0000 - val_loss: 1675548688384.0000\n","Epoch 31/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5968262856704.0000 - val_loss: 1675013914624.0000\n","Epoch 32/100\n","136/136 [==============================] - 7s 50ms/step - loss: 5967473278976.0000 - val_loss: 1674477043712.0000\n","Epoch 33/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5966681079808.0000 - val_loss: 1673944891392.0000\n","Epoch 34/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5965890977792.0000 - val_loss: 1673408413696.0000\n","Epoch 35/100\n","136/136 [==============================] - 7s 50ms/step - loss: 5965100875776.0000 - val_loss: 1672873639936.0000\n","Epoch 36/100\n","136/136 [==============================] - 7s 48ms/step - loss: 5964309725184.0000 - val_loss: 1672337555456.0000\n","Epoch 37/100\n","136/136 [==============================] - 6s 45ms/step - loss: 5963521720320.0000 - val_loss: 1671805796352.0000\n","Epoch 38/100\n","136/136 [==============================] - 6s 45ms/step - loss: 5962734764032.0000 - val_loss: 1671269711872.0000\n","Epoch 39/100\n","136/136 [==============================] - 7s 48ms/step - loss: 5961947807744.0000 - val_loss: 1670738083840.0000\n","Epoch 40/100\n","136/136 [==============================] - 7s 48ms/step - loss: 5961157705728.0000 - val_loss: 1670203047936.0000\n","Epoch 41/100\n","136/136 [==============================] - 6s 47ms/step - loss: 5960368652288.0000 - val_loss: 1669670633472.0000\n","Epoch 42/100\n","136/136 [==============================] - 7s 48ms/step - loss: 5959579074560.0000 - val_loss: 1669135204352.0000\n","Epoch 43/100\n","136/136 [==============================] - 6s 44ms/step - loss: 5958789496832.0000 - val_loss: 1668601217024.0000\n","Epoch 44/100\n","136/136 [==============================] - 8s 60ms/step - loss: 5958001491968.0000 - val_loss: 1668067098624.0000\n","Epoch 45/100\n","136/136 [==============================] - 7s 50ms/step - loss: 5957214011392.0000 - val_loss: 1667533897728.0000\n","Epoch 46/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5956426530816.0000 - val_loss: 1667002793984.0000\n","Epoch 47/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5955636428800.0000 - val_loss: 1666470772736.0000\n","Epoch 48/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5954851045376.0000 - val_loss: 1665935212544.0000\n","Epoch 49/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5954058321920.0000 - val_loss: 1665401880576.0000\n","Epoch 50/100\n","136/136 [==============================] - 7s 50ms/step - loss: 5953272414208.0000 - val_loss: 1664868286464.0000\n","Epoch 51/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5952483885056.0000 - val_loss: 1664333250560.0000\n","Epoch 52/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5951695880192.0000 - val_loss: 1663802540032.0000\n","Epoch 53/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5950911545344.0000 - val_loss: 1663270518784.0000\n","Epoch 54/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5950123016192.0000 - val_loss: 1662737842176.0000\n","Epoch 55/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5949333962752.0000 - val_loss: 1662207918080.0000\n","Epoch 56/100\n","136/136 [==============================] - 8s 62ms/step - loss: 5948546482176.0000 - val_loss: 1661673406464.0000\n","Epoch 57/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5947760574464.0000 - val_loss: 1661140992000.0000\n","Epoch 58/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5946973093888.0000 - val_loss: 1660607660032.0000\n","Epoch 59/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5946184564736.0000 - val_loss: 1660077473792.0000\n","Epoch 60/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5945399705600.0000 - val_loss: 1659544272896.0000\n","Epoch 61/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5944614322176.0000 - val_loss: 1659012120576.0000\n","Epoch 62/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5943825268736.0000 - val_loss: 1658480492544.0000\n","Epoch 63/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5943039885312.0000 - val_loss: 1657949388800.0000\n","Epoch 64/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5942251356160.0000 - val_loss: 1657419988992.0000\n","Epoch 65/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5941465972736.0000 - val_loss: 1656886394880.0000\n","Epoch 66/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5940679540736.0000 - val_loss: 1656353062912.0000\n","Epoch 67/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5939892584448.0000 - val_loss: 1655823663104.0000\n","Epoch 68/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5939104055296.0000 - val_loss: 1655291510784.0000\n","Epoch 69/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5938316050432.0000 - val_loss: 1654755950592.0000\n","Epoch 70/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5937530667008.0000 - val_loss: 1654227337216.0000\n","Epoch 71/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5936744759296.0000 - val_loss: 1653693743104.0000\n","Epoch 72/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5935958327296.0000 - val_loss: 1653162639360.0000\n","Epoch 73/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5935172419584.0000 - val_loss: 1652631273472.0000\n","Epoch 74/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5934386511872.0000 - val_loss: 1652103053312.0000\n","Epoch 75/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5933602701312.0000 - val_loss: 1651569852416.0000\n","Epoch 76/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5932816793600.0000 - val_loss: 1651040452608.0000\n","Epoch 77/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5932032983040.0000 - val_loss: 1650510659584.0000\n","Epoch 78/100\n","136/136 [==============================] - 8s 62ms/step - loss: 5931244453888.0000 - val_loss: 1649981784064.0000\n","Epoch 79/100\n","136/136 [==============================] - 6s 44ms/step - loss: 5930458546176.0000 - val_loss: 1649449631744.0000\n","Epoch 80/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5929671589888.0000 - val_loss: 1648918003712.0000\n","Epoch 81/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5928886206464.0000 - val_loss: 1648385196032.0000\n","Epoch 82/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5928100823040.0000 - val_loss: 1647856058368.0000\n","Epoch 83/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5927318061056.0000 - val_loss: 1647326789632.0000\n","Epoch 84/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5926531629056.0000 - val_loss: 1646797258752.0000\n","Epoch 85/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5925748342784.0000 - val_loss: 1646267990016.0000\n","Epoch 86/100\n","136/136 [==============================] - 7s 55ms/step - loss: 5924962435072.0000 - val_loss: 1645737803776.0000\n","Epoch 87/100\n","136/136 [==============================] - 8s 57ms/step - loss: 5924179148800.0000 - val_loss: 1645209321472.0000\n","Epoch 88/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5923395338240.0000 - val_loss: 1644681232384.0000\n","Epoch 89/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5922612051968.0000 - val_loss: 1644149997568.0000\n","Epoch 90/100\n","136/136 [==============================] - 6s 43ms/step - loss: 5921827717120.0000 - val_loss: 1643620204544.0000\n","Epoch 91/100\n","136/136 [==============================] - 7s 54ms/step - loss: 5921043382272.0000 - val_loss: 1643091722240.0000\n","Epoch 92/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5920257998848.0000 - val_loss: 1642562322432.0000\n","Epoch 93/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5919473664000.0000 - val_loss: 1642032660480.0000\n","Epoch 94/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5918688280576.0000 - val_loss: 1641502736384.0000\n","Epoch 95/100\n","136/136 [==============================] - 7s 52ms/step - loss: 5917908140032.0000 - val_loss: 1640977006592.0000\n","Epoch 96/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5917120135168.0000 - val_loss: 1640443936768.0000\n","Epoch 97/100\n","136/136 [==============================] - 7s 51ms/step - loss: 5916336324608.0000 - val_loss: 1639916371968.0000\n","Epoch 98/100\n","136/136 [==============================] - 6s 41ms/step - loss: 5915554086912.0000 - val_loss: 1639388545024.0000\n","Epoch 99/100\n","136/136 [==============================] - 8s 61ms/step - loss: 5914772897792.0000 - val_loss: 1638860718080.0000\n","Epoch 100/100\n","136/136 [==============================] - 6s 42ms/step - loss: 5913989611520.0000 - val_loss: 1638333022208.0000\n"]}]},{"cell_type":"code","metadata":{"id":"IqwLl7JeLp3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680081024649,"user_tz":-540,"elapsed":1982,"user":{"displayName":"쑹쨩","userId":"02967438864685795672"}},"outputId":"b2a797f2-3deb-435e-a630-c2f697c86fdd"},"source":["# Q1. 테스트 셋에서의 RMSE를 출력하여라.\n","from sklearn.metrics import mean_squared_error as MSE\n","\n","y_pred = model.predict(x_test)\n","MSE(y_test, y_pred) ** 0.5"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 2s 2s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["1585038.1002198977"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"aZeY_ZOUQ722"},"source":["## Q1. Conv1D + Bidirectional\n","\n","**[공식문서 적극 참고](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)**\n","Bidirectional)**\n","1. 적절한 인풋 레이어를 구성할 것\n","2. Conv1D\n","    * 32개 필터 사용할 것\n","    * window_size(filter_size)는 10일\n","    * activation은 swish\n","3. MaxPool1D\n","    * 필터 사이즈(window size)는 2일\n","4. Bidirectional 레이어\n","    * forward layer : LSTM, 히든스테이트 노드 24개\n","    * backward layer : GRU, 히든스테이트 노드 16개\n","4. Bidirectional 레이어\n","    * forward layer : LSTM, 히든스테이트 노드 24개\n","    * backward layer : GRU, 히든스테이트 노드 24개\n","4. 플래튼\n","6. Fully Connected : 노드 256개, swish\n","5. 회귀를 위한 적절한 아웃풋 레이어\n"]},{"cell_type":"code","metadata":{"id":"HCAPYUvVE--W"},"source":["####################\n","## Your Code Here ##\n","####################\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJaqGuLME--W"},"source":["## 학습도 시킬 것\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RZiHyiNE--W"},"source":["# Q1. 테스트 셋에서의 RMSE를 출력하여라.\n","\n"],"execution_count":null,"outputs":[]}]}