{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0_0_Seq2Seq_Basics.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOu70D+s203DfbdZlGUl6Ky"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IGlWW9jN5e9a"},"source":["# Sequence to Sequence (a.k.a. seq2seq)\r\n","\r\n","**학습목표**\r\n","* Encoder Decoder 구조를 이해하고 구현할 줄 안다.\r\n","* Seq2Seq에 필요한 전처리를 이해한다.\r\n","\r\n","![이런거](https://raw.githubusercontent.com/KerasKorea/KEKOxTutorial/master/media/28_1.png)\r\n","---------------------------------\r\n","edu.rayleigh@gmail.com\r\n","Special Thanks to : 숙번님 ( [봉수골 개발자 이선비](https://www.youtube.com/channel/UCOAyyrvi7tnCAz7RhH98QCQ) )"]},{"cell_type":"code","metadata":{"id":"ObiFUtaC8pHk"},"source":["# !wget http://www.manythings.org/anki/fra-eng.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VeblRhOP9CxX"},"source":["# import zipfile\r\n","# fra_eng = zipfile.ZipFile('fra-eng.zip')\r\n","# fra_eng.extractall()\r\n","# fra_eng.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5VTApAubX8bb"},"source":["!wget https://raw.githubusercontent.com/L1aoXingyu/seq2seq-translation/master/data/eng-fra.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-A2Uf0V-y-W"},"source":["import pandas as pd\r\n","# temp = pd.read_table('fra.txt', names=['Eng', 'Fra', 'License'])\r\n","temp = pd.read_table('eng-fra.txt', names=['Eng', 'Fra'])\r\n","temp.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0n8j-x_j8rTj"},"source":["temp.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bI1y7O_OSAVY"},"source":["# 너무 많으므로 50000개 문장만 진행하자."]},{"cell_type":"code","metadata":{"id":"Pm522fcSSDdH"},"source":["# ## 끔찍한 결과를 볼 수 있다.\r\n","# temp = temp.sample(n=50000, replace=False, random_state=2021)\r\n","\r\n","temp = temp.iloc[:50000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHEF-9_1AHF2"},"source":["eng_sent = temp['Eng'].tolist()\r\n","fra_sent = temp['Fra'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qntWaQtjSo0U"},"source":["print(eng_sent[100])\r\n","print(fra_sent[100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aPU1CYykAdXU"},"source":["# 데이터 준비\r\n","0. 단어와 구두점 사이 공백 만들기\r\n","1. sos 와 eos\r\n","1. tokenizing, idx_seq, padding"]},{"cell_type":"markdown","metadata":{"id":"Fj9FhCC9IgaO"},"source":["## 0. 단어와 구두점 사이 공백 만들기\r\n"]},{"cell_type":"code","metadata":{"id":"Wi0kBaQRIjwB"},"source":["import unicodedata\r\n","import re\r\n","def unicode_to_ascii(s):\r\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\r\n","      if unicodedata.category(c) != 'Mn')\r\n","def preprocess_sentence(sent):\r\n","    # 위에서 구현한 함수를 내부적으로 호출\r\n","    sent = unicode_to_ascii(sent.lower())\r\n","\r\n","    # 단어와 구두점 사이에 공백을 만듭니다.\r\n","    # Ex) \"he is a boy.\" => \"he is a boy .\"\r\n","    sent = re.sub(r\"([?.!,'¿])\", r\" \\1 \", sent)\r\n","\r\n","    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\r\n","    sent = re.sub(r\"[^a-zA-Z!.?']+\", r\" \", sent)\r\n","\r\n","    sent = re.sub(r\"\\s+\", \" \", sent)\r\n","    return sent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjAoavx2IsTj"},"source":["preprocess_sentence(\"I'm just a poor boy.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjr_CjB6JFlj"},"source":["eng_sent = [ preprocess_sentence(sent) for sent in eng_sent ]\r\n","fra_sent = [ preprocess_sentence(sent) for sent in fra_sent ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MIrvRfKJOB3"},"source":["print(eng_sent[100])\r\n","print(fra_sent[100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3LR2w3i3EJ6c"},"source":["## 1. sos 와 eos\r\n","1. sos : start of speech\r\n","2. eos : end of speech"]},{"cell_type":"code","metadata":{"id":"RFkzszhyDq6E"},"source":["fra_sent = [f\"<sos> {fra} <eos>\" for fra in fra_sent]\r\n","fra_sent[100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrEhHIQlEU8F"},"source":["## 2. Tokenizing, idx_seq, padding"]},{"cell_type":"code","metadata":{"id":"4-C-zqpSEewe"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbf-Y--TEj7Y"},"source":["# Tokenizing\r\n","tokenizer_en = Tokenizer(filters=\"\", lower=True)\r\n","tokenizer_en.fit_on_texts(eng_sent)\r\n","tokenizer_fr = Tokenizer(filters=\"\", lower=True)\r\n","tokenizer_fr.fit_on_texts(fra_sent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvDe7BY-F3jB"},"source":["# Index Sequence\r\n","eng_seq = tokenizer_en.texts_to_sequences(eng_sent)\r\n","fra_seq = tokenizer_fr.texts_to_sequences(fra_sent)\r\n","\r\n","print(eng_seq[100])\r\n","print(fra_seq[100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sadRsDiaJ7p5"},"source":["# padding\r\n","eng_pad = pad_sequences(eng_seq) # 최대 문장 길이에 패딩에 맞춰지게 됨.\r\n","fra_pad = pad_sequences(fra_seq)\r\n","\r\n","print(eng_pad.shape)\r\n","print(fra_pad.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW-52MAiKjLW"},"source":["# tokenizer에서 0 index가 구성되어있지 않지만, \r\n","# pad_sequence에서 pad의 의미로 0을 사용하고 있어서, 전체 사이즈를 구할 때, +1을 해준다.\r\n","\r\n","eng_vocab_size = len(tokenizer_en.word_index) + 1\r\n","fra_vocab_size = len(tokenizer_fr.word_index) + 1\r\n","print(\"영어 단어 집합의 크기: {:d}\\n프랑스어 단어 집합의 크기: {:d}\".format(eng_vocab_size, fra_vocab_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"faI0SNWRKr7O"},"source":["# 모델링!"]},{"cell_type":"code","metadata":{"id":"daSlqB5TK-ua"},"source":["import tensorflow as tf\r\n","from tensorflow.keras.layers import Input, Embedding, GRU"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHPZhTlXK8Gy"},"source":["# 혹시 이미 그려둔 그래프가 있다면 날려줘!\r\n","tf.keras.backend.clear_session()\r\n","\r\n","# 영어 단어 집합의 크기 : 5965, (50000, 11)\r\n","# 프랑스어 단어 집합의 크기 : 10406, (50000, 19)\r\n","# 프랑스어 문장은 길이가 19이지만,\r\n","# 디코더의 인풋으로 넣을때는 맨 뒤의 <eos>를 떼고 길이 18의 문장을\r\n","# 디코더의 아웃풋은 맨 앞의 <eos>를 떼고 길이 18의 문장으로 준비해야 함.\r\n","\r\n","# Encoder\r\n","enc_X = tf.keras.layers.Input(shape=[eng_pad.shape[1]])\r\n","enc_E = tf.keras.layers.Embedding(eng_vocab_size, 64)(enc_X) # 토큰수, 차원수\r\n","enc_S_full, enc_S = tf.keras.layers.GRU(256, return_sequences=True, return_state=True)(enc_E)\r\n","## 물론 지금은 enc_S_full은 사용하지 않는다.\r\n","\r\n","# Decoder\r\n","dec_X = tf.keras.layers.Input(shape=[fra_pad.shape[1]-1])\r\n","dec_E = tf.keras.layers.Embedding(fra_vocab_size, 64)(dec_X) # 토큰수, 차원수\r\n","dec_H = tf.keras.layers.GRU(256, return_sequences=True)(dec_E, initial_state=enc_S)\r\n","dec_H = tf.keras.layers.Dense(256, activation=\"swish\")(dec_H) # 없어도 상관은 없는 부분.\r\n","dec_Y = tf.keras.layers.Dense(fra_vocab_size, activation=\"softmax\")(dec_H) # 매시점에서, 어떤 단어가 타당할지 분류 문제로 푸는 것\r\n","\r\n","model = tf.keras.models.Model([enc_X, dec_X], dec_Y)\r\n","# 텍스트는 index이고(원핫인코딩을 안했고)\r\n","# 아웃풋레이어는 분류문제 처럼 노드가 준비되어 있다면\r\n","# sparse categorical crossentropy\r\n","model.compile(loss='sparse_categorical_crossentropy',\r\n","              optimizer = 'rmsprop',\r\n","              metrics=['accuracy'])\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceSiL3RxNuQE"},"source":["# decoder의 인풋은 마지막 <eos>를 뗀다.\r\n","# decoder의 아웃풋 학습시엔 처음의 <sos>를 뗀다.\r\n","model.fit([eng_pad, fra_pad[:, :-1]], fra_pad[:, 1:], shuffle=True, \r\n","          batch_size=128, epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cHRRLWbOO8Z"},"source":["import numpy as np\r\n","\r\n","# 영어 단어 집합의 크기 : 5965, (50000, 11)\r\n","# 프랑스어 단어 집합의 크기 : 10406, (50000, 19)\r\n","\r\n","def translate(eng):\r\n","    # eng => index => pad\r\n","    eng_seq = tokenizer_en.texts_to_sequences([eng])\r\n","    eng_pad = tf.keras.preprocessing.sequence.pad_sequences(eng_seq, maxlen=11)\r\n","\r\n","    fra = []\r\n","    for n in range(19-1):\r\n","        # fra => index => pad\r\n","        fra_seq = tokenizer_fr.texts_to_sequences([['<sos>'] + fra])\r\n","        fra_pad = tf.keras.preprocessing.sequence.pad_sequences(fra_seq, maxlen=19-1)\r\n","\r\n","        fra_next = model.predict([eng_pad, fra_pad])\r\n","\r\n","        # onehot -> index -> word\r\n","        fra = [tokenizer_fr.index_word[i] for i in np.argmax(fra_next[0], axis=1) if i != 0]\r\n","        # 번역된 word 선택\r\n","        fra = fra[:n+1]\r\n","        \r\n","        if fra[-1] == '<eos>':\r\n","            break\r\n","\r\n","    return fra"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUglKyFVQW4y"},"source":["import random\r\n","\r\n","# 랜덤 10개\r\n","indices = list(range(50000))\r\n","random.shuffle(indices)\r\n","\r\n","for n in indices[:10]:\r\n","    print(f\"영어: {eng_sent[n]}\\n불어: {fra_sent[n]}\")\r\n","    print(f\"번역: {' '.join(translate(eng_sent[n])[:-1])}\")\r\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mZSvqaFUdwV"},"source":[""],"execution_count":null,"outputs":[]}]}