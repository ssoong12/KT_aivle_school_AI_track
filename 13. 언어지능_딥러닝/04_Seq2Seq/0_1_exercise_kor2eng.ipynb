{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IGlWW9jN5e9a"},"source":["# Sequence to Sequence (a.k.a. seq2seq)\n","\n","**학습목표**\n","* Encoder Decoder 구조를 이해하고 구현할 줄 안다.\n","* Seq2Seq에 필요한 전처리를 이해한다.\n","* **데이터 부족**과, **긴 문장**을 겪어본다.\n","\n","![이런거](https://raw.githubusercontent.com/KerasKorea/KEKOxTutorial/master/media/28_1.png)\n","---------------------------------\n","edu.rayleigh@gmail.com\n","Special Thanks to : 숙번님 ( [봉수골 개발자 이선비](https://www.youtube.com/channel/UCOAyyrvi7tnCAz7RhH98QCQ) )"]},{"cell_type":"code","metadata":{"id":"ObiFUtaC8pHk"},"source":["!wget http://www.manythings.org/anki/kor-eng.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VeblRhOP9CxX"},"source":["import zipfile\n","kor_eng = zipfile.ZipFile('kor-eng.zip')\n","kor_eng.extractall()\n","kor_eng.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-A2Uf0V-y-W"},"source":["import pandas as pd\n","temp = pd.read_table('kor.txt', names=['Eng', 'Kor', 'license'])\n","temp.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0n8j-x_j8rTj"},"source":["temp.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHEF-9_1AHF2"},"source":["eng_sent = temp['Eng'].tolist()\n","kor_sent = temp['Kor'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qntWaQtjSo0U"},"source":["print(eng_sent[1000])\n","print(kor_sent[1000])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aPU1CYykAdXU"},"source":["# 데이터 준비\n","0. 단어와 구두점 사이 공백 만들기\n","1. sos 와 eos\n","1. tokenizing, idx_seq, padding"]},{"cell_type":"markdown","metadata":{"id":"Fj9FhCC9IgaO"},"source":["## 0. 단어와 구두점 사이 공백 만들기\n"]},{"cell_type":"code","metadata":{"id":"Wi0kBaQRIjwB"},"source":["import unicodedata\n","import re\n","def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","def eng_preprocessor(sent):\n","    # 위에서 구현한 함수를 내부적으로 호출\n","    sent = unicode_to_ascii(sent.lower())\n","\n","    # 단어와 구두점 사이에 공백을 만듭니다.\n","    # Ex) \"he is a boy.\" => \"he is a boy .\"\n","    sent = re.sub(r\"([?.!,'¿])\", r\" \\1 \", sent)\n","\n","    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n","    sent = re.sub(r\"[^a-zA-Z!.?']+\", r\" \", sent)\n","\n","    sent = re.sub(r\"\\s+\", \" \", sent)\n","    return sent\n","\n","def kor_preprocessor(sent):\n","    # 위에서 구현한 함수를 내부적으로 호출\n","    sent = unicode_to_ascii(sent.lower())\n","\n","    # 단어와 구두점 사이에 공백을 만듭니다.\n","    # Ex) \"he is a boy.\" => \"he is a boy .\"\n","    sent = re.sub(r\"([?.!,'¿])\", r\" \\1 \", sent)\n","\n","    sent = re.sub(r\"\\s+\", \" \", sent)\n","    return sent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjAoavx2IsTj"},"source":["eng_preporcessor(\"I'm just a poor boy.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjr_CjB6JFlj"},"source":["eng_sent = [ eng_preprocessor(sent) for sent in eng_sent ]\n","kor_sent = [ kor_preprocessor(sent) for sent in kor_sent ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2MIrvRfKJOB3"},"source":["print(eng_sent[1000])\n","print(kor_sent[1000])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3LR2w3i3EJ6c"},"source":["## 1. sos 와 eos\n","1. sos : start of speech\n","2. eos : end of speech"]},{"cell_type":"code","metadata":{"id":"RFkzszhyDq6E"},"source":["######################\n","### Your Code here ###\n","######################\n","\n","## 영어 문장 전 후에 <sos>와 <eos>를 추가할 것\n","## 띄어쓰기 주의!\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrEhHIQlEU8F"},"source":["## 2. Tokenizing, idx_seq, padding"]},{"cell_type":"code","metadata":{"id":"4-C-zqpSEewe"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbf-Y--TEj7Y"},"source":["######################\n","### Your Code here ###\n","######################\n","\n","# Tokenizing    # 한국어는 lower = False\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvDe7BY-F3jB"},"source":["######################\n","### Your Code here ###\n","######################\n","\n","# Index Sequence\n","eng_seq = \n","kor_seq = \n","\n","print(eng_seq[1000])\n","print(kor_seq[1000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sadRsDiaJ7p5"},"source":["######################\n","### Your Code here ###\n","######################\n","## 최대 문장 길이에 맞춰지도록 할 것.\n","# padding\n","eng_pad = \n","kor_pad = \n","\n","print(eng_pad.shape)\n","print(kor_pad.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW-52MAiKjLW"},"source":["# tokenizer에서 0 index가 구성되어있지 않지만, \n","# pad_sequence에서 pad의 의미로 0을 사용하고 있어서, 전체 사이즈를 구할 때, +1을 해준다.\n","\n","eng_vocab_size = len(tokenizer_en.word_index) + 1\n","kor_vocab_size = len(tokenizer_kr.word_index) + 1\n","print(\"영어 단어 집합의 크기: {:d}\\n한국어 단어 집합의 크기: {:d}\".format(eng_vocab_size, kor_vocab_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"faI0SNWRKr7O"},"source":["# 모델링!\n","\n","1. 모든 임베딩 레이어는 128개 차원으로 구성.\n","2. 인코더도 디코더도 GRU, 히든스테이트 512로 구성.\n","3. 디코더의 GRU 뒤에는 Fully Conneceted layer 사용. 노드 512개\n","4. 적절한 아웃풋레이어\n","    * 매 시점, 가장 적절한 단어가 무엇일지 분류 한다고 생각하면 됨!"]},{"cell_type":"code","metadata":{"id":"daSlqB5TK-ua"},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Embedding, GRU"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHPZhTlXK8Gy"},"source":["######################\n","### Your Code here ###\n","######################\n","\n","# 혹시 이미 그려둔 그래프가 있다면 날려줘!\n","tf.keras.backend.clear_session()\n","\n","# 한국어 단어 집합의 크기 : 5551, (50000, 95)\n","# 영어 단어 집합의 크기 : 2484, (50000, 104)\n","\n","\n","# Encoder\n","\n","\n","# Decoder\n","\n","\n","# Model\n","\n","\n","\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceSiL3RxNuQE"},"source":["######################\n","### Your Code here ###\n","######################\n","## 학습 시킬 것!\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cHRRLWbOO8Z"},"source":["import numpy as np\n","\n","# 한국어 단어 집합의 크기 : 5551, (50000, 95)\n","# 영어 단어 집합의 크기 : 2484, (50000, 104)\n","\n","def translate(kor):\n","    # eng => index => pad\n","    kor_seq = tokenizer_kr.texts_to_sequences([kor])\n","    kor_pad = tf.keras.preprocessing.sequence.pad_sequences(kor_seq, maxlen=95)\n","\n","    eng = []\n","    for n in range(104-1):\n","        # kor => index => pad\n","        eng_seq = tokenizer_en.texts_to_sequences([['<sos>'] + eng])\n","        eng_pad = tf.keras.preprocessing.sequence.pad_sequences(eng_seq, maxlen=104-1)\n","        eng_next = model.predict([kor_pad, eng_pad])\n","\n","        # onehot -> index -> word\n","        eng = [tokenizer_en.index_word[i] for i in np.argmax(eng_next[0], axis=1) if i != 0]\n","        # 번역된 word 선택\n","        eng = eng[:n+1]\n","        \n","        if eng[-1] == '<eos>':\n","            break\n","\n","    return eng"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUglKyFVQW4y"},"source":["import random\n","\n","# 랜덤 10개\n","indices = list(range(3648))\n","random.shuffle(indices)\n","\n","for n in indices[:10]:\n","    print(f\"한국어: {kor_sent[n]}\\n영어: {eng_sent[n]}\")\n","    print(f\"번역: {' '.join(translate(kor_sent[n])[:-1])}\")\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mZSvqaFUdwV"},"source":[],"execution_count":null,"outputs":[]}]}