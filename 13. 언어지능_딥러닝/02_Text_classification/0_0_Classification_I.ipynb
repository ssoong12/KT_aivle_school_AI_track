{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0_0_Classification_I.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOnMKsf4re//sTYWFcu57tN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PkvvOR8sQPkz"},"source":["# Sentiment Classification & Embedding I\r\n","\r\n","* Before RNN & Embedding Layer"]},{"cell_type":"markdown","metadata":{"nbpresent":{"id":"56bb3cba-260c-4ebe-9ed6-b995b4c72aa3"},"id":"KOVIdeDodOxF"},"source":["# 01. What data we use?"]},{"cell_type":"code","metadata":{"id":"Bwug5phXe2-E"},"source":["import pandas as pd\n","import zipfile as zf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-LTelsJg93P"},"source":["!curl -o labels.txt https://raw.githubusercontent.com/RayleighKim/Example_datasets/master/sentiment_data/labels.txt\n","!curl -o reviews.zip https://raw.githubusercontent.com/RayleighKim/Example_datasets/master/sentiment_data/reviews.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDIOicxTfWbp"},"source":["zf_r = zf.ZipFile('reviews.zip')\n","\n","labels = pd.read_csv('labels.txt', header=None, names = ['labels'])\n","reviews = pd.read_csv(zf_r.open('reviews.txt'), header=None, names = ['reviews'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZNNDKybgOZd"},"source":["labels.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwQCX9ZBJEuE"},"source":["labels['y'] = 0\n","labels.loc[labels['labels']=='positive', ['y']] = 1\n","\n","labels.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RSMOzo3eiamE"},"source":["reviews.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cBqcbrkf4HA"},"source":["y = labels[['y']].values\n","labels = labels['labels'].tolist()\n","reviews = reviews['reviews'].tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"nbpresent":{"id":"eba2b193-0419-431e-8db9-60f34dd3fe83"},"id":"E4nMA0qDdOxG"},"source":["def preview(i):\n","    print(labels[i] + \"  :  \" + reviews[i][:80] + \"...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gq6mPqHBdOxJ"},"source":["len(reviews)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"nbpresent":{"id":"bb95574b-21a0-4213-ae50-34363cf4f87f"},"id":"u9EGYZ81dOxO"},"source":["reviews[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"nbpresent":{"id":"e0408810-c424-4ed4-afb9-1735e9ddbd0a"},"id":"zDo5rpKmdOxR"},"source":["labels[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYOFyToHdOxU"},"source":["# Discussion : 무엇으로 무엇을 어떻게 예측하려 하는가?"]},{"cell_type":"code","metadata":{"nbpresent":{"id":"e67a709f-234f-4493-bae6-4fb192141ee0"},"id":"DpZDonNEdOxU"},"source":["print(\"labels.txt  :  reviews.txt\")\n","print(\"--------------------------\")\n","preview(2490)\n","preview(12786)\n","preview(6267)\n","preview(24965)\n","preview(11947)\n","preview(2312)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMUr_AXuRIHA"},"source":["# Q1. Tokenizer를 이용하여, TFIDF vector를 만들 것.\r\n","\r\n","* 만들고 나서 x와 y를 제작한다.\r\n","    * reviews --> x\r\n","    * labels --> y\r\n","    * 80%는 트레이닝 셋으로, 20%는 테스트 셋으로\r\n","\r\n","\r\n","\r\n","* Bag of Words : Binary ver\r\n","> texts_to_matrix(reviews, mode='binary')\r\n","* Bag of Words : Counting ver\r\n","> texts_to_matrix(reviews, mode='count')\r\n","* Tf-IDF\r\n","> texts_to_matrix(reviews, mode='tfidf')"]},{"cell_type":"code","metadata":{"id":"0_0O-tj7RpVw"},"source":["from tensorflow import keras\n","import tensorflow.keras.preprocessing.text import Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOY7rUZYRwsX"},"source":["t = Tokenizer()\n","t.fit_on_texts(reviews)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vj_kdoZnOPuV"},"source":["print(len(t.word_counts))\n","print(t.document_count)\n","print(t.word_index)\n","print(t.word_docs) # 그 단어가 포함된 문서의 수"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJyY7uztfWaa"},"source":["use_len = 10000 ## 이거 없이 다 한다면 터진다.\r\n","\r\n","x = t.texts_to_matrix(reviews[:use_len])\r\n","y = y[:use_len]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlt-3QG1Ojsh"},"source":["# 모델링!\n","\n"]},{"cell_type":"code","metadata":{"id":"wVNf88yjgQm7"},"source":["import matplotlib.pyplot as plt\r\n","\r\n","from tensorflow.keras import layers, models\r\n","from sklearn.metrics import accuracy_score, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zC-17dIBMwq"},"source":["# 혹시 이미 그려둔 그래프가 있다면 날려줘!\n","keras.backend.clear_session()\n","\n","# model에 순차적으로 레이어를 쌓아가겠다는 의도!\n","model = keras.models.Sequential()\n","\n","# 인풋을 받아, weight을 곱하고, bias를 더해주고\n","# activation은 없애보자!\n","\n","model.add( layers.Dense(2048, input_shape=(74073,), activation='swish')  )\n","model.add( layers.Dense(1024, activation='swish')  )\n","\n","model.add( layers.Dense(1, activation = 'sigmoid')     )\n","\n","adam = keras.optimizers.Adam(lr = 0.0001)\n","\n","# 컴파일 해주렴!\n","model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics =['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Voq9A_ESPBC1"},"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","es = EarlyStopping(monitor = 'val_loss',\n","                  min_delta = 0, # 개선되고 있다고 판단하기 위한 최소 변화량\n","                  patience = 4, # 개선 없는 epoch 얼마나 기달려 줄거야?\n","                  verbose = 1\n","                  )\n","\n","history = model.fit(x[:-2000],y[:-2000], batch_size = 2048, epochs=10, verbose=1,\n","                   validation_split = 0.2, callbacks=[es], )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-JuxYn-HPxcC"},"source":["test_x , test_y = x[-2000:], y[-2000:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQsUQ6zO5m2M"},"source":["performance_test = model.evaluate(test_x, test_y, batch_size = 512)\n","\n","print('Test Loss : {:.6f},  Test Accuracy : {:.3f}%'.format(performance_test[0], performance_test[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjF_zORh7ENc"},"source":["if not isinstance(history, dict):\n","    history = history.history\n","\n","plt.plot(history['accuracy'])\n","plt.plot(history['val_accuracy'])\n","plt.title('Accuracy : Training vs Validation')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc=0)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7Sm3eg67FRL"},"source":["if not isinstance(history, dict):\n","    history = history.history\n","\n","plt.plot(history['loss'])\n","plt.plot(history['val_loss'])\n","plt.title('Loss : Training vs Validation')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc=0)\n","plt.show()"],"execution_count":null,"outputs":[]}]}