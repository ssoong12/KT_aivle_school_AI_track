{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQFmelHqMtFn"
   },
   "source": [
    "# **미니프로젝트 4차 1대1 문의 내용 유형 분류기**\n",
    "# 단계1 : 데이터 탐색\n",
    "\n",
    "### 문제 정의\n",
    "> 1:1 문의 내용 분류 문제<br>\n",
    "> 1. 문의 내용 분석\n",
    "> 2. 문의 내용 분류 모델 성능 평가\n",
    "### 학습 데이터\n",
    "> * 1:1 문의 내용 데이터 : train.csv\n",
    "\n",
    "### 변수 소개\n",
    "> * text : 문의 내용\n",
    "> * label : 문의 유형\n",
    "\n",
    "### References\n",
    "> * 한국어 처리\n",
    ">> * [konlpy - 한국어 처리 라이브러리](https://konlpy.org/ko/latest/)\n",
    ">> * [한국어 품사 태그 비교표](https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)\n",
    ">> * [한국어 품사 태깅 성능 비교](https://konlpy.org/ko/latest/morph/#comparison-between-pos-tagging-classes)\n",
    ">> * [한국어 시스템 사전](https://konlpy.org/ko/latest/data/#corpora)\n",
    "\n",
    "> * 자연어 처리\n",
    ">> * [NLTK](https://www.nltk.org/book/)\n",
    ">> * [gensim](https://radimrehurek.com/gensim/)\n",
    ">> * [Google guide](https://developers.google.com/machine-learning/guides/text-classification/step-2)\n",
    ">> * [WordCloud](https://amueller.github.io/word_cloud/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWToio0hXNVS"
   },
   "source": [
    "## 1. 개발 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_rCs78QJZmI"
   },
   "source": [
    "* 세부 요구사항\n",
    "  - 기본적으로 필요한 라이브러리를 import 하도록 코드가 작성되어 있습니다.\n",
    "  - 필요하다고 판단되는 라이브러리를 추가하세요.\n",
    "  - konlpy, mecab 설치 후 형태소 분석 함수 생성\n",
    "  - mecab 설치할 때 윈도우 pc에서 설치는 다른 방법으로 진행\n",
    "  - 윈도우 환경일 경우 KoNLPy의 라이브러리 설치가 제대로 이루어지지 않을 수 있습니다\n",
    "  - 윈도우 설치를 위한 참고 링크\n",
    "    - https://liveyourit.tistory.com/56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리부터 설치할께요.\n",
    "!pip install konlpy pandas seaborn gensim wordcloud python-mecab-ko wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mecab import MeCab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import wget, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 한글 글꼴 설정(Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"malgun.ttf\"): \n",
    "    wget.download(\"https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\")\n",
    "if 'malgun' not in fm.fontManager.findfont(\"Malgun Gothic\"):\n",
    "    fm.fontManager.addfont(\"malgun.ttf\")\n",
    "if plt.rcParams['font.family']!= [\"Malgun Gothic\"]:\n",
    "    plt.rcParams['font.family']= [font for font in fm.fontManager.ttflist if 'malgun.ttf' in font.fname][-1].name\n",
    "plt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\n",
    "assert plt.rcParams['font.family'] == [\"Malgun Gothic\"], \"한글 폰트가 설정되지 않았습니다.\"\n",
    "FONT_PATH = \"malgun.ttf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. 자바 경로 설정(Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = \"C:\\Program Files\\Java\\jdk-19\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 한글 글꼴 설정(Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT_PATH = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=FONT_PATH, size=10).get_name()\n",
    "print(font_name)\n",
    "plt.rcParams['font.family']=font_name\n",
    "assert plt.rcParams['font.family'] == [font_name], \"한글 폰트가 설정되지 않았습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0yzHcH6fdzJ"
   },
   "source": [
    "### 1-4. 구글드라이브 연결(Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd0SPbYdfhS9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA-eHXdwBDCz"
   },
   "source": [
    "## 2.데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OkifR5rnr2D"
   },
   "source": [
    "* 주어진 데이터\n",
    " - 학습 및 검증용 데이터 : train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSbNaqTSZpDZ"
   },
   "source": [
    "### 2-1. 데이터 로딩\n",
    "\n",
    "* 다음 데이터를 불러옵니다.\n",
    "    * 학습 및 검증용 데이터 : train.csv\n",
    "    * shape를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSbNaqTSZpDZ"
   },
   "source": [
    "### 2-2. 데이터 확인하기\n",
    "* 문의 유형 분포 확인\n",
    "* data type, 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U2AP87sMbD8"
   },
   "source": [
    "## 3.데이터 탐색하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 문의 내용 길이 분포\n",
    "\n",
    "* 문의 내용의 길이 분포를 확인합니다.\n",
    "* 최소, 최대 길이의 index 및 문의 내용을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 명사 추출하고 품사 태깅하기\n",
    "\n",
    "* konlpy, MeCab을 활용하여 품사 태깅, 명사 추출하기.\n",
    "* 2개 이상의 형태소 분석기를 Time analysis 해보기\n",
    "* Number of Samples / Number of words per sample 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. nltk로 텍스트 분석하기\n",
    "* 3-2.의 명사 추출, 품사 태깅한 데이터를 nltk Text로 변경하기\n",
    "* 각각 token 갯수 / 전체 token 비율(Type-Token Ratio) 알아보기\n",
    "* 각각 Frequency 확인 및 분포 시각화\n",
    "* 명사 추출한 데이터에서 코드, 웹, 이론, 원격, 시스템 등 관심있는 각 단어에 분석해보기\n",
    "* 각각 collocation 확인\n",
    "* 최소 길이 문의 내용에 대해 grammar pasing 해보기(아래 코드 활용)\n",
    "\n",
    "\n",
    "```python\n",
    "grammar = \"\"\"\n",
    "NP: {<N.*>*<Suffix>?}   # Noun phrase\n",
    "VP: {<V.*>*}            # Verb phrase\n",
    "AP: {<A.*>*}            # Adjective phrase\n",
    "\"\"\"\n",
    "NP_grammar = \"NP: {<Adjective>*<Noun>*}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Wordcloud 만들기\n",
    "* 명사 추출/형태소 분석 결과를 이용하여 WordCloud 객체를 생성\n",
    "* 최대 글꼴 크기(max_font_size), 최대 단어 수(max_words), 배경 색상(background_color)\n",
    "* 상대 크기비율(relative_scaling), 그림 크기(width, height), 폰트 경로(font_path=FONT_PATH) 등 설정\n",
    "* .generate 메서드를 연결하여 WordCloud에 사용될 텍스트 데이터 생성\n",
    "* plt.imshow(  ) 를 통해 화면에 보여집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문의 유형에 따른 분석\n",
    "### 4-1. *3. 데이터 탐색*에서 실행한 알고리즘을 문의 유형에 따라 실행할 수 있는 함수를 개발합니다.\n",
    "* 아래 text_analysis 함수를 개발합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis(df, label=None):\n",
    "    \"\"\"\n",
    "    label에 따라서 분석, label이 None인 경우 전체 분석\n",
    "    데이터 확인, 데이터 길이 시각화\n",
    "    명사 추출하여 분석, 워드 클라우드 시각화\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame object.\n",
    "    label : Label for analysis, default=None.\n",
    "    - If label is None, Entire data are analyzed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : None\n",
    "\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 각각의 문의 유형에 대해 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in sorted(train_df.label.unique()):\n",
    "    text_analysis(train_df,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단계2 : 데이터 전처리\n",
    "> * Tokenization(Konlpy...)\n",
    "> * Vectorization(N-grams, Sequence)\n",
    "> * [Google guide](https://developers.google.com/machine-learning/guides/text-classification/step-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 준비\n",
    "### 5-1. label 아래 형식으로 처리\n",
    "```python\n",
    "label_dict = {\n",
    "    '코드1': 0,\n",
    "    '코드2': 0,\n",
    "    '웹': 1,\n",
    "    '이론': 2,\n",
    "    '시스템 운영': 3,\n",
    "    '원격': 4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. 특수문자 제거(Optional)\n",
    "- [정규표현식 이용](https://docs.python.org/3/howto/regex.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. N-grams (sklearn)\n",
    "> * [scikit-learn working with text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#)\n",
    "> * [scikit-learn text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "> * [한글 자료](https://datascienceschool.net/03%20machine%20learning/03.01.03%20Scikit-Learn%EC%9D%98%20%EB%AC%B8%EC%84%9C%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%8A%A5.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. Sequence (keras, whatever)\n",
    "> * [keras text classification](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
    "> * [tensorflow text classification](https://www.tensorflow.org/tutorials/keras/text_classification)\n",
    "* tokenizer.fit_on_texts() : 주어진 텍스트 데이터에 대해 단어 사전 생성\n",
    "* 문장 길이 분포 살펴보기\n",
    "* 문장 길이를 구해서 기초통계량, histogram, boxplot을 그려보고 적절한 개수를 결정합니다.\n",
    "* tokenizer.texts_to_sequences() : 주어진 텍스트 데이터를 정수 시퀀스로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-4. Word2Vec (gensim, optional) \n",
    "> * [gensim-word2vec-tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py)\n",
    "> * [tensorflow-word2vec-tutorial](https://www.tensorflow.org/tutorials/text/word2vec?hl=ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 함수는 제공합니다.\n",
    "def get_sent_embeddings(model, embedding_size, tokenized_words):\n",
    "\n",
    "    # 단어 임베딩 및 n_words의 크기가 0인 feature_vec 배열을 0으로 초기화합니다. \n",
    "    # 또한 model.wv.index2word를 사용하여 Word2Vec 모델의 어휘에 단어 세트를 생성합니다.\n",
    "    feature_vec = np.zeros((embedding_size,), dtype='float32')\n",
    "    n_words = 0\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "\n",
    "    # 토큰화된 문장의 각 단어를 반복하고 Word2Vec 모델의 어휘에 존재하는지 확인합니다. \n",
    "    # 그렇다면 n_words가 증가하고 단어의 임베딩이 feature_vec에 추가됩니다.\n",
    "    for word in tokenized_words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])\n",
    "    \n",
    "    # Word2Vec 모델의 어휘에 있는 입력 문장에 단어가 있는지 확인합니다. \n",
    "    # 있다면 feature_vec를 n_words로 나누어 입력 문장의 평균 임베딩을 구합니다.\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "        \n",
    "    return feature_vec\n",
    "def get_dataset(sentences, model, num_features):\n",
    "\n",
    "    # 각 문장에 대한 임베딩을 보유할 dataset이라는 빈 목록을 초기화합니다.\n",
    "    dataset = list()\n",
    "\n",
    "    # 문장의 각 문장을 반복하고 앞에서 설명한 get_sent_embeddings() 함수를 사용하여 문장에 대한 평균 임베딩을 생성합니다. \n",
    "    # 결과 문장 임베딩이 데이터 세트 목록에 추가됩니다.\n",
    "    for sent in sentences:\n",
    "        dataset.append(get_sent_embeddings(model, num_features, sent))\n",
    "\n",
    "    # 루프에서 생성된 문장 임베딩을 sent_embedding_vectors라는 2차원 배열에 쌓습니다. \n",
    "    sent_embedding_vectors = np.stack(dataset)\n",
    "    \n",
    "    return sent_embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-5. 데이터 저장하기\n",
    "* N-gram, Sequence로 처리한 데이터 저장\n",
    "* sparse data에 대해서는 scipy.sparse.save_npz 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
